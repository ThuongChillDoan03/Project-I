{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc08e9ce",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-04-24T02:54:49.144554Z",
     "iopub.status.busy": "2024-04-24T02:54:49.143658Z",
     "iopub.status.idle": "2024-04-24T02:54:52.621585Z",
     "shell.execute_reply": "2024-04-24T02:54:52.620714Z"
    },
    "papermill": {
     "duration": 3.488589,
     "end_time": "2024-04-24T02:54:52.624033",
     "exception": false,
     "start_time": "2024-04-24T02:54:49.135444",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "import matplotlib\n",
    "# matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73dbb50",
   "metadata": {
    "papermill": {
     "duration": 0.005833,
     "end_time": "2024-04-24T02:54:52.636272",
     "exception": false,
     "start_time": "2024-04-24T02:54:52.630439",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Clone Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88317b2c",
   "metadata": {
    "papermill": {
     "duration": 0.00565,
     "end_time": "2024-04-24T02:54:52.647859",
     "exception": false,
     "start_time": "2024-04-24T02:54:52.642209",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**ENCODER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "414f4e33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-24T02:54:52.661244Z",
     "iopub.status.busy": "2024-04-24T02:54:52.660819Z",
     "iopub.status.idle": "2024-04-24T02:54:52.667052Z",
     "shell.execute_reply": "2024-04-24T02:54:52.666141Z"
    },
    "papermill": {
     "duration": 0.015385,
     "end_time": "2024-04-24T02:54:52.669295",
     "exception": false,
     "start_time": "2024-04-24T02:54:52.653910",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"Encodes the static & dynamic states using 1d Convolution.\"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv = nn.Conv1d(input_size, hidden_size, kernel_size=1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.conv(input)\n",
    "        return output  # (batch, hidden_size, seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a677455",
   "metadata": {
    "papermill": {
     "duration": 0.006379,
     "end_time": "2024-04-24T02:54:52.683242",
     "exception": false,
     "start_time": "2024-04-24T02:54:52.676863",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**ATTENTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2047791",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-24T02:54:52.696930Z",
     "iopub.status.busy": "2024-04-24T02:54:52.696644Z",
     "iopub.status.idle": "2024-04-24T02:54:52.705289Z",
     "shell.execute_reply": "2024-04-24T02:54:52.704398Z"
    },
    "papermill": {
     "duration": 0.017575,
     "end_time": "2024-04-24T02:54:52.707207",
     "exception": false,
     "start_time": "2024-04-24T02:54:52.689632",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "###____Tensor-3D.size() = (Depth, Row, col)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"Calculates attention over the input nodes given the current state.\"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "        # W processes features from static decoder elements\n",
    "        self.v = nn.Parameter(torch.zeros((1, 1, hidden_size),\n",
    "                                          device=device, requires_grad=True))\n",
    "\n",
    "        self.W = nn.Parameter(torch.zeros((1, hidden_size, 3 * hidden_size),\n",
    "                                          device=device, requires_grad=True))\n",
    "        ###_Initilization_Tensor-3D\n",
    "\n",
    "    def forward(self, static_hidden, dynamic_hidden, decoder_hidden):\n",
    "\n",
    "        batch_size, hidden_size, _ = static_hidden.size()\n",
    "\n",
    "        hidden = decoder_hidden.unsqueeze(2).expand_as(static_hidden)       ###(BXHXS): S = Seq_len: Sequence_Length\n",
    "        hidden = torch.cat((static_hidden, dynamic_hidden, hidden), 1)  ## Connect 3 tensors along the horizontal axis (Axis 1)\n",
    "\n",
    "        # Broadcast some dimensions so we can do batch-matrix-multiply\n",
    "\n",
    "        v = self.v.expand(batch_size, 1, hidden_size)\n",
    "        W = self.W.expand(batch_size, hidden_size, -1)\n",
    "\n",
    "        attns = torch.bmm(v, torch.tanh(torch.bmm(W, hidden)))\n",
    "        attns = F.softmax(attns, dim=2)  # (batch, seq_len)\n",
    "        return attns "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ca6c5d",
   "metadata": {
    "papermill": {
     "duration": 0.006201,
     "end_time": "2024-04-24T02:54:52.719602",
     "exception": false,
     "start_time": "2024-04-24T02:54:52.713401",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**POINTER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9580509",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-24T02:54:52.733259Z",
     "iopub.status.busy": "2024-04-24T02:54:52.732610Z",
     "iopub.status.idle": "2024-04-24T02:54:52.744464Z",
     "shell.execute_reply": "2024-04-24T02:54:52.743593Z"
    },
    "papermill": {
     "duration": 0.020833,
     "end_time": "2024-04-24T02:54:52.746400",
     "exception": false,
     "start_time": "2024-04-24T02:54:52.725567",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Pointer(nn.Module):\n",
    "    \"\"\"Calculates the next state given the previous state and input embeddings.\"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size, num_layers=1, dropout=0.2):\n",
    "        super(Pointer, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Used to calculate probability of selecting next state\n",
    "        self.v = nn.Parameter(torch.zeros((1, 1, hidden_size),\n",
    "                                          device=device, requires_grad=True))\n",
    "\n",
    "        self.W = nn.Parameter(torch.zeros((1, hidden_size, 2 * hidden_size),\n",
    "                                          device=device, requires_grad=True))\n",
    "\n",
    "        # Used to compute a representation of the current decoder output\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, num_layers,\n",
    "                          batch_first=True,\n",
    "                          dropout=dropout if num_layers > 1 else 0)\n",
    "        self.encoder_attn = Attention(hidden_size)\n",
    "\n",
    "        self.drop_rnn = nn.Dropout(p=dropout)\n",
    "        self.drop_hh = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, static_hidden, dynamic_hidden, decoder_hidden, last_hh):\n",
    "\n",
    "        rnn_out, last_hh = self.gru(decoder_hidden.transpose(2, 1), last_hh)\n",
    "        rnn_out = rnn_out.squeeze(1)\n",
    "\n",
    "        # Always apply dropout on the RNN output\n",
    "        rnn_out = self.drop_rnn(rnn_out)\n",
    "        if self.num_layers == 1:\n",
    "            # If > 1 layer dropout is already applied\n",
    "            last_hh = self.drop_hh(last_hh)\n",
    "\n",
    "        # Given a summary of the output, find an  input context\n",
    "        enc_attn = self.encoder_attn(static_hidden, dynamic_hidden, rnn_out)\n",
    "        context = enc_attn.bmm(static_hidden.permute(0, 2, 1))  # (B, 1, num_feats)\n",
    "\n",
    "        # Calculate the next output using Batch-matrix-multiply ops\n",
    "        context = context.transpose(1, 2).expand_as(static_hidden)\n",
    "        energy = torch.cat((static_hidden, context), dim=1)  # (B, num_feats, seq_len)\n",
    "\n",
    "        v = self.v.expand(static_hidden.size(0), -1, -1)\n",
    "        W = self.W.expand(static_hidden.size(0), -1, -1)\n",
    "\n",
    "        probs = torch.bmm(v, torch.tanh(torch.bmm(W, energy))).squeeze(1)\n",
    "\n",
    "        return probs, last_hh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee4f3f4",
   "metadata": {
    "papermill": {
     "duration": 0.005887,
     "end_time": "2024-04-24T02:54:52.758565",
     "exception": false,
     "start_time": "2024-04-24T02:54:52.752678",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**MODEL OF TSP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64057c6f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-24T02:54:52.771926Z",
     "iopub.status.busy": "2024-04-24T02:54:52.771631Z",
     "iopub.status.idle": "2024-04-24T02:54:52.792452Z",
     "shell.execute_reply": "2024-04-24T02:54:52.791602Z"
    },
    "papermill": {
     "duration": 0.029819,
     "end_time": "2024-04-24T02:54:52.794349",
     "exception": false,
     "start_time": "2024-04-24T02:54:52.764530",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DRL4TSP(nn.Module):\n",
    "    \"\"\"Defines the main Encoder, Decoder, and Pointer combinatorial models.\n",
    "    Parameters\n",
    "    ----------\n",
    "    static_size: int\n",
    "        Defines how many features are in the static elements of the model\n",
    "        (e.g. 2 for (x, y) coordinates)\n",
    "    dynamic_size: int > 1\n",
    "        Defines how many features are in the dynamic elements of the model\n",
    "        (e.g. 2 for the VRP which has (load, demand) attributes. The TSP doesn't\n",
    "        have dynamic elements, but to ensure compatility with other optimization\n",
    "        problems, assume we just pass in a vector of zeros.\n",
    "    hidden_size: int\n",
    "        Defines the number of units in the hidden layer for all static, dynamic,\n",
    "        and decoder output units.\n",
    "    update_fn: function or None\n",
    "        If provided, this method is used to calculate how the input dynamic\n",
    "        elements are updated, and is called after each 'point' to the input element.\n",
    "    mask_fn: function or None\n",
    "        Allows us to specify which elements of the input sequence are allowed to\n",
    "        be selected. This is useful for speeding up training of the networks,\n",
    "        by providing a sort of 'rules' guidlines to the algorithm. If no mask\n",
    "        is provided, we terminate the search after a fixed number of iterations\n",
    "        to avoid tours that stretch forever\n",
    "    num_layers: int\n",
    "        Specifies the number of hidden layers to use in the decoder RNN\n",
    "    dropout: float\n",
    "        Defines the dropout rate for the decoder\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, static_size, dynamic_size, hidden_size,\n",
    "                 update_fn=None, mask_fn=None, num_layers=1, dropout=0.):\n",
    "        super(DRL4TSP, self).__init__()\n",
    "\n",
    "        if dynamic_size < 1:\n",
    "            raise ValueError(':param dynamic_size: must be > 0, even if the '\n",
    "                             'problem has no dynamic elements')\n",
    "\n",
    "        self.update_fn = update_fn\n",
    "        self.mask_fn = mask_fn\n",
    "\n",
    "        # Define the encoder & decoder models\n",
    "        self.static_encoder = Encoder(static_size, hidden_size)\n",
    "        self.dynamic_encoder = Encoder(dynamic_size, hidden_size)\n",
    "        self.decoder = Encoder(static_size, hidden_size)\n",
    "        self.pointer = Pointer(hidden_size, num_layers, dropout)\n",
    "\n",
    "        for p in self.parameters():\n",
    "            if len(p.shape) > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "        # Used as a proxy initial state in the decoder when not specified\n",
    "        self.x0 = torch.zeros((1, static_size, 1), requires_grad=True, device=device)\n",
    "\n",
    "    def forward(self, static, dynamic, decoder_input=None, last_hh=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        static: Array of size (batch_size, feats, num_cities)\n",
    "            Defines the elements to consider as static. For the TSP, this could be\n",
    "            things like the (x, y) coordinates, which won't change\n",
    "        dynamic: Array of size (batch_size, feats, num_cities)\n",
    "            Defines the elements to consider as static. For the VRP, this can be\n",
    "            things like the (load, demand) of each city. If there are no dynamic\n",
    "            elements, this can be set to None\n",
    "        decoder_input: Array of size (batch_size, num_feats)\n",
    "            Defines the outputs for the decoder. Currently, we just use the\n",
    "            static elements (e.g. (x, y) coordinates), but this can technically\n",
    "            be other things as well\n",
    "        last_hh: Array of size (batch_size, num_hidden)\n",
    "            Defines the last hidden state for the RNN\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size, input_size, sequence_size = static.size()\n",
    "\n",
    "        if decoder_input is None:\n",
    "            decoder_input = self.x0.expand(batch_size, -1, -1)\n",
    "\n",
    "        # Always use a mask - if no function is provided, we don't update it\n",
    "        mask = torch.ones(batch_size, sequence_size, device=device)\n",
    "\n",
    "        # Structures for holding the output sequences\n",
    "        tour_idx, tour_logp = [], []\n",
    "        max_steps = sequence_size if self.mask_fn is None else 1000\n",
    "\n",
    "        # Static elements only need to be processed once, and can be used across\n",
    "        # all 'pointing' iterations. When / if the dynamic elements change,\n",
    "        # their representations will need to get calculated again.\n",
    "        static_hidden = self.static_encoder(static)\n",
    "        dynamic_hidden = self.dynamic_encoder(dynamic)\n",
    "\n",
    "        for _ in range(max_steps):\n",
    "\n",
    "            if not mask.byte().any():\n",
    "                break\n",
    "\n",
    "            # ... but compute a hidden rep for each element added to sequence\n",
    "            decoder_hidden = self.decoder(decoder_input)\n",
    "\n",
    "            probs, last_hh = self.pointer(static_hidden,\n",
    "                                          dynamic_hidden,\n",
    "                                          decoder_hidden, last_hh)\n",
    "            probs = F.softmax(probs + mask.log(), dim=1)\n",
    "\n",
    "            # When training, sample the next step according to its probability.\n",
    "            # During testing, we can take the greedy approach and choose highest\n",
    "            if self.training:\n",
    "                m = torch.distributions.Categorical(probs)\n",
    "\n",
    "                # Sometimes an issue with Categorical & sampling on GPU; See:\n",
    "                # https://github.com/pemami4911/neural-combinatorial-rl-pytorch/issues/5\n",
    "                ptr = m.sample()\n",
    "                while not torch.gather(mask, 1, ptr.data.unsqueeze(1)).byte().all():\n",
    "                    ptr = m.sample()\n",
    "                logp = m.log_prob(ptr)\n",
    "            else:\n",
    "                prob, ptr = torch.max(probs, 1)  # Greedy\n",
    "                logp = prob.log()\n",
    "\n",
    "            # After visiting a node update the dynamic representation\n",
    "            if self.update_fn is not None:\n",
    "                dynamic = self.update_fn(dynamic, ptr.data)\n",
    "                dynamic_hidden = self.dynamic_encoder(dynamic)\n",
    "\n",
    "                # Since we compute the VRP in minibatches, some tours may have\n",
    "                # number of stops. We force the vehicles to remain at the depot\n",
    "                # in these cases, and logp := 0\n",
    "                is_done = dynamic[:, 1].sum(1).eq(0).float()\n",
    "                logp = logp * (1. - is_done)\n",
    "\n",
    "            # And update the mask so we don't re-visit if we don't need to\n",
    "            if self.mask_fn is not None:\n",
    "                mask = self.mask_fn(mask, dynamic, ptr.data).detach()\n",
    "\n",
    "            tour_logp.append(logp.unsqueeze(1))\n",
    "            tour_idx.append(ptr.data.unsqueeze(1))\n",
    "\n",
    "            decoder_input = torch.gather(static, 2,\n",
    "                                         ptr.view(-1, 1, 1)\n",
    "                                         .expand(-1, input_size, 1)).detach()\n",
    "\n",
    "        tour_idx = torch.cat(tour_idx, dim=1)  # (batch_size, seq_len)\n",
    "        tour_logp = torch.cat(tour_logp, dim=1)  # (batch_size, seq_len)\n",
    "\n",
    "        return tour_idx, tour_logp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca30cd4b",
   "metadata": {
    "papermill": {
     "duration": 0.006202,
     "end_time": "2024-04-24T02:54:52.806717",
     "exception": false,
     "start_time": "2024-04-24T02:54:52.800515",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**MOTSP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cce05db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-24T02:54:52.820300Z",
     "iopub.status.busy": "2024-04-24T02:54:52.820047Z",
     "iopub.status.idle": "2024-04-24T02:54:52.837845Z",
     "shell.execute_reply": "2024-04-24T02:54:52.837000Z"
    },
    "papermill": {
     "duration": 0.026724,
     "end_time": "2024-04-24T02:54:52.839827",
     "exception": false,
     "start_time": "2024-04-24T02:54:52.813103",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TSPDataset(Dataset):\n",
    "\n",
    "    def __init__(self, size=50, num_samples=1e6, seed=None):\n",
    "        super(TSPDataset, self).__init__()\n",
    "\n",
    "        if seed is None:\n",
    "            seed = np.random.randint(123456789)\n",
    "\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        self.dataset = torch.rand((num_samples, 4, size))\n",
    "        self.dynamic = torch.zeros(num_samples, 1, size)\n",
    "        self.num_nodes = size\n",
    "        self.size = num_samples\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # (static, dynamic, start_loc)\n",
    "        return (self.dataset[idx], self.dynamic[idx], [])\n",
    "\n",
    "\n",
    "def update_mask(mask, dynamic, chosen_idx):\n",
    "    \"\"\"Marks the visited city, so it can't be selected a second time.\"\"\"\n",
    "    mask.scatter_(1, chosen_idx.unsqueeze(1), 0)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def reward(static, tour_indices, w1=1, w2=0):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    static: torch.FloatTensor containing static (e.g. x, y) data\n",
    "    tour_indices: torch.IntTensor of size (batch_size, num_cities)\n",
    "    Returns\n",
    "    -------\n",
    "    Euclidean distance between consecutive nodes on the route. of size\n",
    "    (batch_size, num_cities)\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the indices back into a tour\n",
    "    idx = tour_indices.unsqueeze(1).expand_as(static)\n",
    "    tour = torch.gather(static.data, 2, idx).permute(0, 2, 1)\n",
    "\n",
    "    # Make a full tour by returning to the start\n",
    "    y = torch.cat((tour, tour[:, :1]), dim=1)\n",
    "    # first 2 is xy coordinate, third column is another obj\n",
    "    y_dis = y[:, :, :2]\n",
    "    y_dis2 = y[:, :, 2:]\n",
    "\n",
    "    # Euclidean distance between each consecutive point\n",
    "    tour_len = torch.sqrt(torch.sum(torch.pow(y_dis[:, :-1] - y_dis[:, 1:], 2), dim=2))\n",
    "    obj1 = tour_len.sum(1).detach()\n",
    "\n",
    "    tour_len2 = torch.sqrt(torch.sum(torch.pow(y_dis2[:, :-1] - y_dis2[:, 1:], 2), dim=2))\n",
    "    obj2 = tour_len2.sum(1).detach()\n",
    "\n",
    "    obj = w1*obj1 + w2*obj2\n",
    "    return obj, obj1, obj2\n",
    "\n",
    "\n",
    "\n",
    "def render(static, tour_indices, save_path):\n",
    "    \"\"\"Plots the found tours.\"\"\"\n",
    "\n",
    "    plt.close('all')\n",
    "\n",
    "    num_plots = 3 if int(np.sqrt(len(tour_indices))) >= 3 else 1\n",
    "\n",
    "    _, axes = plt.subplots(nrows=num_plots, ncols=num_plots,\n",
    "                           sharex='col', sharey='row')\n",
    "\n",
    "    if num_plots == 1:\n",
    "        axes = [[axes]]\n",
    "    axes = [a for ax in axes for a in ax]\n",
    "\n",
    "    for i, ax in enumerate(axes):\n",
    "\n",
    "        # Convert the indices back into a tour\n",
    "        idx = tour_indices[i]\n",
    "        if len(idx.size()) == 1:\n",
    "            idx = idx.unsqueeze(0)\n",
    "\n",
    "        # End tour at the starting index\n",
    "        idx = idx.expand(static.size(1), -1)\n",
    "        idx = torch.cat((idx, idx[:, 0:1]), dim=1)\n",
    "\n",
    "        data = torch.gather(static[i].data, 1, idx).cpu().numpy()\n",
    "\n",
    "        #plt.subplot(num_plots, num_plots, i + 1)\n",
    "        ax.plot(data[0], data[1], zorder=1)\n",
    "        ax.scatter(data[0], data[1], s=4, c='r', zorder=2)\n",
    "        ax.scatter(data[0, 0], data[1, 0], s=20, c='k', marker='*', zorder=3)\n",
    "\n",
    "        ax.set_xlim(0, 1)\n",
    "        ax.set_ylim(0, 1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, bbox_inches='tight', dpi=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2915021c",
   "metadata": {
    "papermill": {
     "duration": 0.005672,
     "end_time": "2024-04-24T02:54:52.851574",
     "exception": false,
     "start_time": "2024-04-24T02:54:52.845902",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**TRAIN**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f7ff01",
   "metadata": {
    "papermill": {
     "duration": 0.005752,
     "end_time": "2024-04-24T02:54:52.863151",
     "exception": false,
     "start_time": "2024-04-24T02:54:52.857399",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Critic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ea8e152",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-24T02:54:52.876160Z",
     "iopub.status.busy": "2024-04-24T02:54:52.875890Z",
     "iopub.status.idle": "2024-04-24T02:54:52.884307Z",
     "shell.execute_reply": "2024-04-24T02:54:52.883432Z"
    },
    "papermill": {
     "duration": 0.017142,
     "end_time": "2024-04-24T02:54:52.886113",
     "exception": false,
     "start_time": "2024-04-24T02:54:52.868971",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class StateCritic(nn.Module):\n",
    "    \"\"\"Estimates the problem complexity.\n",
    "    This is a basic module that just looks at the log-probabilities predicted by\n",
    "    the encoder + decoder, and returns an estimate of complexity\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, static_size, dynamic_size, hidden_size):\n",
    "        super(StateCritic, self).__init__()\n",
    "\n",
    "        self.static_encoder = Encoder(static_size, hidden_size)\n",
    "        self.dynamic_encoder = Encoder(dynamic_size, hidden_size)\n",
    "\n",
    "        # Define the encoder & decoder models\n",
    "        self.fc1 = nn.Conv1d(hidden_size * 2, 20, kernel_size=1)\n",
    "        self.fc2 = nn.Conv1d(20, 20, kernel_size=1)\n",
    "        self.fc3 = nn.Conv1d(20, 1, kernel_size=1)\n",
    "\n",
    "        for p in self.parameters():\n",
    "            if len(p.shape) > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def forward(self, static, dynamic):\n",
    "\n",
    "        # Use the probabilities of visiting each\n",
    "        static_hidden = self.static_encoder(static)\n",
    "        dynamic_hidden = self.dynamic_encoder(dynamic)\n",
    "\n",
    "        hidden = torch.cat((static_hidden, dynamic_hidden), 1)\n",
    "\n",
    "        output = F.relu(self.fc1(hidden))\n",
    "        output = F.relu(self.fc2(output))\n",
    "        output = self.fc3(output).sum(dim=2)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025e07aa",
   "metadata": {
    "papermill": {
     "duration": 0.005696,
     "end_time": "2024-04-24T02:54:52.898538",
     "exception": false,
     "start_time": "2024-04-24T02:54:52.892842",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Validate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "285e01c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-24T02:54:52.911858Z",
     "iopub.status.busy": "2024-04-24T02:54:52.911528Z",
     "iopub.status.idle": "2024-04-24T02:54:52.920870Z",
     "shell.execute_reply": "2024-04-24T02:54:52.919979Z"
    },
    "papermill": {
     "duration": 0.018436,
     "end_time": "2024-04-24T02:54:52.922911",
     "exception": false,
     "start_time": "2024-04-24T02:54:52.904475",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def validate(data_loader, actor, reward_fn, w1, w2, render_fn=None, save_dir='.',\n",
    "             num_plot=5):\n",
    "    \"\"\"Used to monitor progress on a validation set & optionally plot solution.\"\"\"\n",
    "\n",
    "    actor.eval()\n",
    "\n",
    "    # if not os.path.exists(save_dir):\n",
    "    #     os.makedirs(save_dir)\n",
    "\n",
    "    rewards = []\n",
    "    obj1s = []\n",
    "    obj2s = []\n",
    "    for batch_idx, batch in enumerate(data_loader):\n",
    "\n",
    "        static, dynamic, x0 = batch\n",
    "\n",
    "        static = static.to(device)\n",
    "        dynamic = dynamic.to(device)\n",
    "        x0 = x0.to(device) if len(x0) > 0 else None\n",
    "\n",
    "        with torch.no_grad():\n",
    "            tour_indices, _ = actor.forward(static, dynamic, x0)\n",
    "\n",
    "        reward, obj1, obj2 = reward_fn(static, tour_indices, w1, w2)\n",
    "\n",
    "        rewards.append(torch.mean(reward.detach()).item())\n",
    "        obj1s.append(torch.mean(obj1.detach()).item())\n",
    "        obj2s.append(torch.mean(obj2.detach()).item())\n",
    "        # if render_fn is not None and batch_idx < num_plot:\n",
    "        #     name = 'batch%d_%2.4f.png'%(batch_idx, torch.mean(reward.detach()).item())\n",
    "        #     path = os.path.join(save_dir, name)\n",
    "        #     render_fn(static, tour_indices, path)\n",
    "\n",
    "    actor.train()\n",
    "    return np.mean(rewards), np.mean(obj1s), np.mean(obj2s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58035ed",
   "metadata": {
    "papermill": {
     "duration": 0.006237,
     "end_time": "2024-04-24T02:54:52.935530",
     "exception": false,
     "start_time": "2024-04-24T02:54:52.929293",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**MAIN TRAIN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe381245",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-24T02:54:52.949237Z",
     "iopub.status.busy": "2024-04-24T02:54:52.948925Z",
     "iopub.status.idle": "2024-04-24T02:54:52.971575Z",
     "shell.execute_reply": "2024-04-24T02:54:52.970686Z"
    },
    "papermill": {
     "duration": 0.031666,
     "end_time": "2024-04-24T02:54:52.973580",
     "exception": false,
     "start_time": "2024-04-24T02:54:52.941914",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(actor, critic, w1, w2, task, num_nodes, train_data, valid_data, reward_fn,\n",
    "          render_fn, batch_size, actor_lr, critic_lr, max_grad_norm,\n",
    "          **kwargs):\n",
    "    \"\"\"Constructs the main actor & critic networks, and performs all training.\"\"\"\n",
    "\n",
    "    now = '%s' % datetime.datetime.now().time()\n",
    "    now = now.replace(':', '_')\n",
    "    bname = \"_transfer\"\n",
    "    save_dir = os.path.join(task+bname, '%d' % num_nodes, 'w_%2.2f_%2.2f' % (w1, w2), now)\n",
    "\n",
    "    checkpoint_dir = os.path.join(save_dir, 'checkpoints')\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "         os.makedirs(checkpoint_dir)\n",
    "\n",
    "    actor_optim = optim.Adam(actor.parameters(), lr=actor_lr)\n",
    "    critic_optim = optim.Adam(critic.parameters(), lr=critic_lr)\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size, True, num_workers=0)\n",
    "    valid_loader = DataLoader(valid_data, batch_size, False, num_workers=0)\n",
    "\n",
    "    best_params = None\n",
    "    best_reward = np.inf\n",
    "    start_total = time.time()\n",
    "    for epoch in range(3):\n",
    "        print(\"epoch %d start:\"% epoch)\n",
    "        actor.train()\n",
    "        critic.train()\n",
    "\n",
    "        times, losses, rewards, critic_rewards = [], [], [], []\n",
    "        obj1s, obj2s = [], []\n",
    "\n",
    "        epoch_start = time.time()\n",
    "        start = epoch_start\n",
    "\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "\n",
    "            static, dynamic, x0 = batch\n",
    "\n",
    "            static = static.to(device)\n",
    "            dynamic = dynamic.to(device)\n",
    "            x0 = x0.to(device) if len(x0) > 0 else None\n",
    "\n",
    "            # Full forward pass through the dataset\n",
    "            tour_indices, tour_logp = actor(static, dynamic, x0)\n",
    "\n",
    "            # Sum the log probabilities for each city in the tour\n",
    "            reward, obj1, obj2 = reward_fn(static, tour_indices, w1, w2)\n",
    "\n",
    "            # Query the critic for an estimate of the reward\n",
    "            critic_est = critic(static, dynamic).view(-1)\n",
    "\n",
    "            advantage = (reward - critic_est)\n",
    "            actor_loss = torch.mean(advantage.detach() * tour_logp.sum(dim=1))\n",
    "            critic_loss = torch.mean(advantage ** 2)\n",
    "\n",
    "            actor_optim.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(actor.parameters(), max_grad_norm)\n",
    "            actor_optim.step()\n",
    "\n",
    "            critic_optim.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(critic.parameters(), max_grad_norm)\n",
    "            critic_optim.step()\n",
    "\n",
    "            critic_rewards.append(torch.mean(critic_est.detach()).item())\n",
    "            rewards.append(torch.mean(reward.detach()).item())\n",
    "            losses.append(torch.mean(actor_loss.detach()).item())\n",
    "            obj1s.append(torch.mean(obj1.detach()).item())\n",
    "            obj2s.append(torch.mean(obj2.detach()).item())\n",
    "            if (batch_idx + 1) % 200 == 0:\n",
    "                print(\"\\n\")\n",
    "                end = time.time()\n",
    "                times.append(end - start)\n",
    "                start = end\n",
    "\n",
    "                mean_loss = np.mean(losses[-100:])\n",
    "                mean_reward = np.mean(rewards[-100:])\n",
    "                mean_obj1 = np.mean(obj1s[-100:])\n",
    "                mean_obj2 = np.mean(obj2s[-100:])\n",
    "                print('  Batch %d/%d, reward: %2.3f, obj1: %2.3f, obj2: %2.3f, loss: %2.4f, took: %2.4fs' %\n",
    "                      (batch_idx, len(train_loader), mean_reward, mean_obj1, mean_obj2, mean_loss,\n",
    "                       times[-1]))\n",
    "\n",
    "        mean_loss = np.mean(losses)\n",
    "        mean_reward = np.mean(rewards)\n",
    "\n",
    "        # Save the weights\n",
    "        # epoch_dir = os.path.join(checkpoint_dir, '%s' % epoch)\n",
    "        # if not os.path.exists(epoch_dir):\n",
    "        #     os.makedirs(epoch_dir)\n",
    "        #\n",
    "        # save_path = os.path.join(epoch_dir, 'actor.pt')\n",
    "        # torch.save(actor.state_dict(), save_path)\n",
    "        #\n",
    "        # save_path = os.path.join(epoch_dir, 'critic.pt')\n",
    "        # torch.save(critic.state_dict(), save_path)\n",
    "\n",
    "        # Save rendering of validation set tours\n",
    "        # valid_dir = os.path.join(save_dir, '%s' % epoch)\n",
    "        mean_valid, mean_obj1_valid, mean_obj2_valid = validate(valid_loader, actor, reward_fn, w1, w2, render_fn,\n",
    "                              '.', num_plot=5)\n",
    "\n",
    "        # Save best model parameters\n",
    "        if mean_valid < best_reward:\n",
    "\n",
    "            best_reward = mean_valid\n",
    "\n",
    "            # save_path = os.path.join(save_dir, 'actor.pt')\n",
    "            # torch.save(actor.state_dict(), save_path)\n",
    "            #\n",
    "            # save_path = os.path.join(save_dir, 'critic.pt')\n",
    "            # torch.save(critic.state_dict(), save_path)\n",
    "            # transfer to next w\n",
    "            main_dir = os.path.join(task+bname, '%d' % num_nodes, 'w_%2.2f_%2.2f' % (w1, w2))\n",
    "            save_path = os.path.join(main_dir, 'actor.pt')\n",
    "            torch.save(actor.state_dict(), save_path)\n",
    "            save_path = os.path.join(main_dir, 'critic.pt')\n",
    "            torch.save(critic.state_dict(), save_path)\n",
    "\n",
    "        print('Mean epoch loss/reward: %2.4f, %2.4f, %2.4f, obj1_valid: %2.3f, obj2_valid: %2.3f. took: %2.4fs '\\\n",
    "              '(%2.4fs / 100 batches)\\n' % \\\n",
    "              (mean_loss, mean_reward, mean_valid, mean_obj1_valid, mean_obj2_valid, time.time() - epoch_start,\n",
    "              np.mean(times)))\n",
    "    print(\"Total run time of epoches: %2.4f\" % (time.time() - start_total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8d1df1",
   "metadata": {
    "papermill": {
     "duration": 0.005786,
     "end_time": "2024-04-24T02:54:52.985389",
     "exception": false,
     "start_time": "2024-04-24T02:54:52.979603",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Trainning Process**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40addd36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-24T02:54:52.998890Z",
     "iopub.status.busy": "2024-04-24T02:54:52.998223Z",
     "iopub.status.idle": "2024-04-24T02:54:53.007511Z",
     "shell.execute_reply": "2024-04-24T02:54:53.006654Z"
    },
    "papermill": {
     "duration": 0.017985,
     "end_time": "2024-04-24T02:54:53.009374",
     "exception": false,
     "start_time": "2024-04-24T02:54:52.991389",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_tsp(args, w1=1, w2=0, checkpoint=None):\n",
    "\n",
    "    # Goals from paper:\n",
    "    # TSP20, 3.97\n",
    "    # TSP50, 6.08\n",
    "    # TSP100, 8.44\n",
    "\n",
    "    STATIC_SIZE = 4 # (x, y)\n",
    "    DYNAMIC_SIZE = 1 # dummy for compatibility\n",
    "\n",
    "    train_data = TSPDataset(args.num_nodes, args.train_size, args.seed)\n",
    "    valid_data = TSPDataset(args.num_nodes, args.valid_size, args.seed + 1)\n",
    "\n",
    "    update_fn = None\n",
    "\n",
    "    actor = DRL4TSP(STATIC_SIZE,\n",
    "                    DYNAMIC_SIZE,\n",
    "                    args.hidden_size,\n",
    "                    update_fn,\n",
    "                    update_mask,\n",
    "                    args.num_layers,\n",
    "                    args.dropout).to(device)\n",
    "\n",
    "    critic = StateCritic(STATIC_SIZE, DYNAMIC_SIZE, args.hidden_size).to(device)\n",
    "\n",
    "    kwargs = vars(args)\n",
    "    kwargs['train_data'] = train_data\n",
    "    kwargs['valid_data'] = valid_data\n",
    "    kwargs['reward_fn'] = reward\n",
    "    kwargs['render_fn'] = render\n",
    "\n",
    "    if not args.test:\n",
    "        train(actor, critic, w1, w2, **kwargs)\n",
    "\n",
    "    test_data = TSPDataset(args.num_nodes, args.valid_size, args.seed + 2)\n",
    "\n",
    "    test_dir = 'test'\n",
    "    test_loader = DataLoader(test_data, args.valid_size, False, num_workers=0)\n",
    "    out = validate(test_loader, actor, reward, w1, w2, render, test_dir, num_plot=5)\n",
    "\n",
    "    print('w1=%2.2f,w2=%2.2f. Average tour length: ' % (w1, w2), out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33db765d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-24T02:54:53.022450Z",
     "iopub.status.busy": "2024-04-24T02:54:53.022198Z",
     "iopub.status.idle": "2024-04-24T02:54:53.036509Z",
     "shell.execute_reply": "2024-04-24T02:54:53.035351Z"
    },
    "papermill": {
     "duration": 0.023639,
     "end_time": "2024-04-24T02:54:53.038979",
     "exception": true,
     "start_time": "2024-04-24T02:54:53.015340",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [-f F] [--seed SEED] [--test] [--task TASK]\n",
      "                             [--nodes NUM_NODES] [--actor_lr ACTOR_LR]\n",
      "                             [--critic_lr CRITIC_LR]\n",
      "                             [--max_grad_norm MAX_GRAD_NORM]\n",
      "                             [--batch_size BATCH_SIZE] [--hidden HIDDEN_SIZE]\n",
      "                             [--dropout DROPOUT] [--layers NUM_LAYERS]\n",
      "                             [--train-size TRAIN_SIZE]\n",
      "                             [--valid-size VALID_SIZE]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --HistoryManager.hist_file=:memory:\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "num_nodes = 100\n",
    "parser = argparse.ArgumentParser(description='Combinatorial Optimization')\n",
    "parser.add_argument('-f')\n",
    "parser.add_argument('--seed', default=12345, type=int)\n",
    "# parser.add_argument('--checkpoint', default=\"tsp/20/w_1_0/20_06_30.888074\")\n",
    "parser.add_argument('--test', action='store_true', default=False)\n",
    "parser.add_argument('--task', default='tsp')\n",
    "parser.add_argument('--nodes', dest='num_nodes', default=num_nodes, type=int)\n",
    "parser.add_argument('--actor_lr', default=5e-4, type=float)\n",
    "parser.add_argument('--critic_lr', default=5e-4, type=float)\n",
    "parser.add_argument('--max_grad_norm', default=2., type=float)\n",
    "parser.add_argument('--batch_size', default=200, type=int)\n",
    "parser.add_argument('--hidden', dest='hidden_size', default=128, type=int)\n",
    "parser.add_argument('--dropout', default=0.1, type=float)\n",
    "parser.add_argument('--layers', dest='num_layers', default=1, type=int)\n",
    "parser.add_argument('--train-size',default=120000, type=int)\n",
    "parser.add_argument('--valid-size', default=1000, type=int)\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "T = 100\n",
    "if args.task == 'tsp':\n",
    "        w2_list = np.arange(T+1)/T\n",
    "        for i in range(0,T+1):\n",
    "            print(\"Current w:%2.2f/%2.2f\"% (1-w2_list[i], w2_list[i]))\n",
    "            if i==0:\n",
    "                # The first subproblem can be trained from scratch. It also can be trained based on a\n",
    "                # single-TSP trained model, where the model can be obtained from everywhere in github\n",
    "                checkpoint = 'tsp_transfer_100run_500000_5epoch_40city/40/w_1.00_0.00'\n",
    "                train_tsp(args, 1, 0, checkpoint)\n",
    "            else:\n",
    "                # Parameter transfer. train based on the parameters of the previous subproblem\n",
    "                checkpoint = 'tsp_transfer/%d/w_%2.2f_%2.2f'%(num_nodes, 1-w2_list[i-1], w2_list[i-1])\n",
    "                train_tsp(args, 1-w2_list[i], w2_list[i], checkpoint)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4859027,
     "sourceId": 8201947,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30698,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7.700414,
   "end_time": "2024-04-24T02:54:54.066862",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-24T02:54:46.366448",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
